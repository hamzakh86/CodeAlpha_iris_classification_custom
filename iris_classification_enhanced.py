#!/usr/bin/env python3
"""
Classification des fleurs d'Iris - Version Personnalis√©e
Auteur: Hamza Khaled
Date: 2025

Ce script impl√©mente une solution compl√®te de classification des fleurs d'Iris
avec des am√©liorations par rapport au projet de r√©f√©rence.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix
import pickle
import warnings
warnings.filterwarnings('ignore')

class IrisClassifier:
    """Classe pour la classification des fleurs d'Iris avec des fonctionnalit√©s avanc√©es."""
    
    def __init__(self):
        self.models = {}
        self.best_model = None
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.results = {}
        
    def load_data(self):
        """Charger et pr√©parer les donn√©es Iris."""
        print("üìä Chargement des donn√©es...")
        # Utiliser le dataset Iris de seaborn pour √©viter les probl√®mes de chemin
        self.df = sns.load_dataset('iris')
        print(f"‚úÖ Donn√©es charg√©es: {self.df.shape[0]} √©chantillons, {self.df.shape[1]} caract√©ristiques")
        return self.df
    
    def explore_data(self):
        """Explorer et visualiser les donn√©es."""
        print("\nüîç Exploration des donn√©es...")
        print(f"Forme du dataset: {self.df.shape}")
        print(f"Colonnes: {list(self.df.columns)}")
        print(f"Types de donn√©es:\n{self.df.dtypes}")
        print(f"Valeurs manquantes:\n{self.df.isnull().sum()}")
        print(f"Distribution des esp√®ces:\n{self.df['species'].value_counts()}")
        
        # Statistiques descriptives
        print(f"\nStatistiques descriptives:\n{self.df.describe()}")
        
    def visualize_data(self):
        """Cr√©er des visualisations avanc√©es."""
        print("\nüìà Cr√©ation des visualisations...")
        
        # Configuration du style
        plt.style.use('seaborn-v0_8')
        fig = plt.figure(figsize=(20, 15))
        
        # 1. Pairplot
        plt.subplot(3, 3, 1)
        sns.pairplot(self.df, hue='species', height=2)
        plt.title('Pairplot des caract√©ristiques')
        
        # 2. Matrice de corr√©lation
        plt.subplot(3, 3, 2)
        correlation_matrix = self.df.select_dtypes(include=[np.number]).corr()
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)
        plt.title('Matrice de corr√©lation')
        
        # 3. Distribution des caract√©ristiques
        plt.subplot(3, 3, 3)
        self.df.select_dtypes(include=[np.number]).hist(bins=20, figsize=(12, 8))
        plt.suptitle('Distribution des caract√©ristiques')
        
        # 4. Boxplots par esp√®ce
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
        for i, feature in enumerate(features):
            ax = axes[i//2, i%2]
            sns.boxplot(data=self.df, x='species', y=feature, ax=ax)
            ax.set_title(f'Distribution de {feature} par esp√®ce')
        
        plt.tight_layout()
        plt.savefig('iris_data_visualization.png', dpi=300, bbox_inches='tight')
        plt.show()
        
    def prepare_data(self):
        """Pr√©parer les donn√©es pour l'entra√Ænement."""
        print("\n‚öôÔ∏è Pr√©paration des donn√©es...")
        
        # S√©parer les caract√©ristiques et les cibles
        X = self.df.drop('species', axis=1)
        y = self.df['species']
        
        # Encoder les labels
        y_encoded = self.label_encoder.fit_transform(y)
        
        # Division train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
        )
        
        # Normalisation des caract√©ristiques
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)
        
        self.X_train, self.X_test = X_train_scaled, X_test_scaled
        self.y_train, self.y_test = y_train, y_test
        
        print(f"‚úÖ Donn√©es pr√©par√©es: {X_train.shape[0]} √©chantillons d'entra√Ænement, {X_test.shape[0]} de test")
        
    def initialize_models(self):
        """Initialiser les mod√®les de machine learning."""
        print("\nü§ñ Initialisation des mod√®les...")
        
        self.models = {
            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
            'K-Nearest Neighbors': KNeighborsClassifier(n_neighbors=5),
            'Support Vector Machine': SVC(random_state=42, probability=True),
            'Naive Bayes': GaussianNB(),
            'Decision Tree': DecisionTreeClassifier(random_state=42),
            'Random Forest': RandomForestClassifier(random_state=42, n_estimators=100),
            'Gradient Boosting': GradientBoostingClassifier(random_state=42)
        }
        
        print(f"‚úÖ {len(self.models)} mod√®les initialis√©s")
        
    def train_and_evaluate(self):
        """Entra√Æner et √©valuer tous les mod√®les."""
        print("\nüèãÔ∏è Entra√Ænement et √©valuation des mod√®les...")
        
        for name, model in self.models.items():
            print(f"\nüìä √âvaluation de {name}...")
            
            # Entra√Ænement
            model.fit(self.X_train, self.y_train)
            
            # Pr√©dictions
            y_pred = model.predict(self.X_test)
            
            # M√©triques
            accuracy = accuracy_score(self.y_test, y_pred)
            precision = precision_score(self.y_test, y_pred, average='weighted')
            recall = recall_score(self.y_test, y_pred, average='weighted')
            f1 = f1_score(self.y_test, y_pred, average='weighted')
            
            # Validation crois√©e
            cv_scores = cross_val_score(model, self.X_train, self.y_train, cv=5)
            
            # Stocker les r√©sultats
            self.results[name] = {
                'model': model,
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'cv_mean': cv_scores.mean(),
                'cv_std': cv_scores.std()
            }
            
            print(f"  Accuracy: {accuracy:.4f}")
            print(f"  Precision: {precision:.4f}")
            print(f"  Recall: {recall:.4f}")
            print(f"  F1-Score: {f1:.4f}")
            print(f"  CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")
            
    def select_best_model(self):
        """S√©lectionner le meilleur mod√®le bas√© sur la validation crois√©e."""
        print("\nüèÜ S√©lection du meilleur mod√®le...")
        
        best_score = 0
        best_name = ""
        
        for name, result in self.results.items():
            if result['cv_mean'] > best_score:
                best_score = result['cv_mean']
                best_name = name
                self.best_model = result['model']
        
        print(f"‚úÖ Meilleur mod√®le: {best_name} (CV Score: {best_score:.4f})")
        return best_name, self.best_model
        
    def hyperparameter_tuning(self, model_name):
        """Optimiser les hyperparam√®tres du meilleur mod√®le."""
        print(f"\nüîß Optimisation des hyperparam√®tres pour {model_name}...")
        
        param_grids = {
            'Random Forest': {
                'n_estimators': [50, 100, 200],
                'max_depth': [None, 10, 20],
                'min_samples_split': [2, 5, 10]
            },
            'Support Vector Machine': {
                'C': [0.1, 1, 10],
                'gamma': ['scale', 'auto', 0.1, 1],
                'kernel': ['rbf', 'linear']
            },
            'K-Nearest Neighbors': {
                'n_neighbors': [3, 5, 7, 9],
                'weights': ['uniform', 'distance']
            }
        }
        
        if model_name in param_grids:
            model = self.models[model_name]
            grid_search = GridSearchCV(
                model, param_grids[model_name], 
                cv=5, scoring='accuracy', n_jobs=-1
            )
            grid_search.fit(self.X_train, self.y_train)
            
            print(f"‚úÖ Meilleurs param√®tres: {grid_search.best_params_}")
            print(f"‚úÖ Meilleur score: {grid_search.best_score_:.4f}")
            
            self.best_model = grid_search.best_estimator_
            return grid_search.best_estimator_
        else:
            print(f"‚ö†Ô∏è Pas d'optimisation d√©finie pour {model_name}")
            return self.best_model
            
    def generate_detailed_report(self):
        """G√©n√©rer un rapport d√©taill√© des r√©sultats."""
        print("\nüìã G√©n√©ration du rapport d√©taill√©...")
        
        # Pr√©dictions du meilleur mod√®le
        y_pred = self.best_model.predict(self.X_test)
        
        # Rapport de classification
        print("\nüìä Rapport de classification:")
        target_names = self.label_encoder.classes_
        print(classification_report(self.y_test, y_pred, target_names=target_names))
        
        # Matrice de confusion
        cm = confusion_matrix(self.y_test, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=target_names, yticklabels=target_names)
        plt.title('Matrice de confusion')
        plt.ylabel('Vraie classe')
        plt.xlabel('Classe pr√©dite')
        plt.savefig('confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        # Comparaison des mod√®les
        results_df = pd.DataFrame(self.results).T
        results_df = results_df[['accuracy', 'precision', 'recall', 'f1_score', 'cv_mean']]
        
        plt.figure(figsize=(12, 8))
        results_df[['accuracy', 'precision', 'recall', 'f1_score']].plot(kind='bar')
        plt.title('Comparaison des performances des mod√®les')
        plt.ylabel('Score')
        plt.xlabel('Mod√®les')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig('model_comparison.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return results_df
        
    def save_model(self, filename='best_iris_model.pkl'):
        """Sauvegarder le meilleur mod√®le."""
        print(f"\nüíæ Sauvegarde du mod√®le dans {filename}...")
        
        model_data = {
            'model': self.best_model,
            'scaler': self.scaler,
            'label_encoder': self.label_encoder,
            'feature_names': ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
        }
        
        with open(filename, 'wb') as f:
            pickle.dump(model_data, f)
        
        print(f"‚úÖ Mod√®le sauvegard√© avec succ√®s!")
        
    def predict_new_sample(self, sepal_length, sepal_width, petal_length, petal_width):
        """Pr√©dire l'esp√®ce d'un nouvel √©chantillon."""
        sample = np.array([[sepal_length, sepal_width, petal_length, petal_width]])
        sample_scaled = self.scaler.transform(sample)
        prediction = self.best_model.predict(sample_scaled)
        probability = self.best_model.predict_proba(sample_scaled)
        
        species = self.label_encoder.inverse_transform(prediction)[0]
        confidence = np.max(probability)
        
        return species, confidence
        
    def run_complete_pipeline(self):
        """Ex√©cuter le pipeline complet de classification."""
        print("üöÄ D√©marrage du pipeline de classification Iris...")
        
        # 1. Charger les donn√©es
        self.load_data()
        
        # 2. Explorer les donn√©es
        self.explore_data()
        
        # 3. Visualiser les donn√©es
        self.visualize_data()
        
        # 4. Pr√©parer les donn√©es
        self.prepare_data()
        
        # 5. Initialiser les mod√®les
        self.initialize_models()
        
        # 6. Entra√Æner et √©valuer
        self.train_and_evaluate()
        
        # 7. S√©lectionner le meilleur mod√®le
        best_name, _ = self.select_best_model()
        
        # 8. Optimiser les hyperparam√®tres
        self.hyperparameter_tuning(best_name)
        
        # 9. G√©n√©rer le rapport
        results_df = self.generate_detailed_report()
        
        # 10. Sauvegarder le mod√®le
        self.save_model()
        
        print("\nüéâ Pipeline termin√© avec succ√®s!")
        return results_df

def main():
    """Fonction principale."""
    classifier = IrisClassifier()
    results = classifier.run_complete_pipeline()
    
    # Test de pr√©diction
    print("\nüß™ Test de pr√©diction:")
    species, confidence = classifier.predict_new_sample(5.1, 3.5, 1.4, 0.2)
    print(f"Pr√©diction: {species} (Confiance: {confidence:.2%})")
    
    return classifier, results

if __name__ == "__main__":
    classifier, results = main()

